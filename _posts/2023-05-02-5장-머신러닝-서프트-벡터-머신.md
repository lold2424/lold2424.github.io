---
layout: single

title: "시스템 분석 및 설계 1장 소개"

date: 2023-05-02 15:30:00 +0900
lastmod:  # sitemap.xml에서 사용됨

author_profile: true

header:
  overlay_image: https://user-images.githubusercontent.com/91832324/230345973-4e85e88e-23c2-4b79-82d9-c7b780584f6b.jpg

  overlay_filter: 0.5 # 투명도

categories: 
  - College SAD

tags: 
    - SAD
    - College

# table of contents
toc: true # 오른쪽 부분에 목차를 자동 생성해준다.
toc_label: "table of content" # toc 이름 설정
toc_icon: "bars" # 아이콘 설정
toc_sticky: true # 마우스 스크롤과 함께 내려갈 것인지 설정
---
# 5. 서포트 벡터 머신

## 1. 선형 SVM 분류

**선형 SVM 아이디**

- 마진 (margin): 클래스를 구분하는 도로의 폭
- 큰 마진 분류(large margin classification): 마진을 최대로 하는 분류

![Untitled](https://user-images.githubusercontent.com/91832324/236998339-1259efdb-5d89-42f9-b6d8-8f5b582e42fe.png)


**서포트 벡터**

- 도로의 양쪽 경계에 위치하는 샘플(아래 그림에서 동그라미 표시됨)
- 서포트 벡터 사이의 간격, 즉 마진이 최대가 되도록 학습
- 특성 스케일을 조정하면 결정경계가 훨씬 좋아짐.

![Untitled 1](https://user-images.githubusercontent.com/91832324/236998340-0ff5c4af-f6e6-4488-993e-94e0b7a49eec.png)

**하드 마진 분류**

- 모든 훈련 샘플이 도로 바깥쪽에 올바르게 분류되도록 하는 마진 분류
- 훈련 세트가 선형적으로 구분되는 경우에만 가능
- 이상치에 민감함
    
    ![Untitled 2](https://user-images.githubusercontent.com/91832324/236998342-411bc72f-a7bf-4f59-9537-104be0b89db0.png)

    |  | 왼편 그래프 | 오른편 그래프 |
    | --- | --- | --- |
    | 이상치 | 타 클래스에 섞임 | 타 클래스에 매우 가까움 |
    | 하드 마진 분류 | 불가능 | 가능하나 일반화 어려움 |

**소프트 마진 분류**

- 마진 오류를 어느 정도 허용하면서 도로의 폭을 최대로 넓게 유지하는 마진 분류
- 마진 오류(margin violation): 결정 경계 도로 위에 또는 경정 경계를 넘어 해당 클래스 반대편에 위치하는 샘플.
소프트 마진 분류의 서프트 벡터 구성
- 꽃잎 길이와 너비 기준의 버지니카와 버시컬러 품종 분류: 소프트 마진 분류만 가능
    
    ![Untitled 3](https://user-images.githubusercontent.com/91832324/236998311-c11d1aa4-797e-41a8-85e8-b6c6124e495a.png)


**예제: 버지니아 품종 여부 판단**

- 사이킷런의 선형 SVM 분류기 `LinearSVC`활용
    - 데이터셋 표준화 스케일링이 중요해서 기본적으로 함께 사용
- 규제 하이퍼파라미터 `C`
    - 작을수록 마진 오류를 강함
    - 클수록 적은 규제: 모델의 자유도 증가,
    마진(결정 경계 도로의 폭)이 작아져서 과대적합 가능성을 키움
    - `C=float(”inf”)`로 지정하면 하드 마진 분류 모델이 됨.
    
    ![Untitled 4](https://user-images.githubusercontent.com/91832324/236998318-d412b605-b7c2-45b0-a0e5-b2850752c416.png)

    |  | 왼편 그래프 | 오른편 그래프 |
    | --- | --- | --- |
    | C | 작게 | 크게 |
    | 마진 | 크게 | 작게 |
    | 분류 | 덜 정교하게 | 보다 정교하게 |

**선형 SVM 분류 지원 모델**

- 선형 분류는 `LinearSVC`모델이 제일 빠르나 `SVC + 선형 커널` 조합도 가능하다.

```python
SVC(kernel="linear", C = 1)
```

- SGDClassifier + hinge 손실함수 + 규제: 규제는 데이터셋 크기(m)에 반비례
- hinge 손실 함수: 어긋난 예측 정도에 비례해 손실값이 선형적으로 커짐.
    
    $$
    \begin{align*}
    J(\mathbf{w}, b) & = \dfrac{1}{2} \mathbf{w}^T \mathbf{w} \,+\, C {\displaystyle \sum_{i=1}^{m}\max\left(0, 1 - t^{(i)} (\mathbf{w}^T \mathbf{x}^{(i)} + b) \right)} \\[.5ex]
    \mathbf{w}^T \mathbf{w} & = w_1^2 + \cdots + w_n^2 \\[.5ex]
    \mathbf{w}^T \mathbf{x}^{(i)} & = w_1 x_1^{(i)} + \cdots + w_n x_n^{(i)}
    \end{align*}
    $$
    
    - $J(\mathbf{w}, b)$: SVM의 손실 함수 $\mathbf{w}$는 가중치 벡터, $b$는 편향이다.
    - $\frac{1}{2} \mathbf{w}^T\mathbf{w}$: 가중치 벡터의 제곱합을 최소화하여 모델의 복잡도를 낮추는 역할을 한다.
    **L2 규제라 한다**
    - $C \sum_{i=1}^{m}\max\left(0, 1 - t^{(i)} (\mathbf{w}^T \mathbf{x}^{(i)} + b) \right)$: 마진 오류(margin error)를 최소화하는 역할을 한다.
        - $C$는 하이퍼파라미터로서, 이 값을 크게하면 마진 오류를 작게 만든다.
        - $t^{(i)}$는 $i$번째 샘플의 레이블이다.
    - $\mathbf{w}^T \mathbf{w}$: 가중치 벡터의 제곱합을 계산
    - $\mathbf{w}^T \mathbf{x}^{(i)}$: 가중치 벡터와 $i$번째 샘플의 특성 벡터의 내적을 계산한다.
    
    이 손실 함수를 최소화하는 가중치 벡터 $\mathbf{w}$와 편향 $b$를 찾아서 SVM을 학습시킵니다.
    
    SVM은 결정 경계(decision boundary)를 찾는 알고리즘으로서, 이 결정 경계는 가장 가까운 샘플과의 거리(margin)가 최대가 되는 초평면(hyperplane)이다.
    
    ![Untitled 5](https://user-images.githubusercontent.com/91832324/236998320-da05c7ee-ca3d-416f-886d-ffa25e39194b.png)


## 2. 비선형 SVM 분류

**1. 특성 추가 + 선형 SVC**

- 다항 특성 활용: 다항 특성을 추가한 후 선형 SVC 적용
- 유사도 특성 활용: 유사도 특성을 추가한 후 선형 SVC 적용

**2. SVC + 커널 트릭**

- 커널 트릭: 새로운 특성을 실제로 추가하지 않으면서 동일한 결과를 유도하는 방식
- 다항 커널
- 가우시안 RBF(방사 기저 함수) 커널

**다항 특성 추가 + 선형 SVC**

- 예제 1: 특성 $x_1$ 하나만 갖는 모델에 새로운 특성 $x_1^2$을 추가한 후 선형 SVM 분류 적용
    
    ![Untitled 6](https://user-images.githubusercontent.com/91832324/236998323-e465b8ce-5de9-4ed4-ac66-3d23fca57a2b.png)

- 다항 특성 + 선형 회귀(4장) 방식과 유사한 기법
    
    $$
    \hat y = \theta_0 + \theta_1x_1 + \theta_22x_1^2
    $$
    
- 예제 2: moons 데이터셋. 마주보는 두 개의 반우너 모양으로 두 개의 클래스로 구분되는 데이터
    
    ![Untitled 7](https://user-images.githubusercontent.com/91832324/236998325-367a6793-85cf-4ee1-8b68-010b7cf70b9e.png)


**SVC + 다항 커널**

- SVM 모델을 훈련시킬 때 다항 특성을 실제로는 추가 하지 않으면서 수학적으로 추가한 효과 활용
- 예제: moons 데이터셋

```python
poly_kernel_svm_clf = make_pipeline(StandardScaler(),
                                    SVC(kernel="poly", degree=3, coef0=1, C=5))
```

**유사도 특성**

- 유사도 함수: 랜드마크(landmark) 샘플과 각 샘플 사이의 유사도(similarity) 측정
- 가우시안 방사 기저 함수(RBF, radial basis function)
    
    $$
    \phi(\mathbf x, m) = \exp(-\gamma\, \lVert \mathbf x - m \lVert^2)
    $$
    
    - $m$: 랜드마크
    - $\gamma$: 랜드마크에서 멀어질수록 0에 수렴하는 속도를 조절
    - $\gamma$값이 클수록 가까운 샘플 선호, 즉 샘플들 사이의 영향을 보다 적게 고려해 모델의 자유도를 높이게 되어 과대적합 위험 커짐.
- 예제

$$
\exp(-5\, \lVert \mathbf x - 1 \lVert^2) \qquad\qquad\qquad\qquad \exp(-100\, \lVert \mathbf x - 1 \lVert^2)
$$

![Untitled 8](https://user-images.githubusercontent.com/91832324/236998326-f2342b46-121d-44e0-90ce-aff3adbbad66.png)

**유사도 특성 추가 + 선형 SVC**

- 예제
    - 랜드마크: -2 와 1
    - $x_2$와 $x_3$: 각각 -2와 1에 대한 가우시안 RBF 함수로 계산한 유사도 특성
    - 화살표가 가리키는 점: $x = -1$
    
    ![Untitled 9](https://user-images.githubusercontent.com/91832324/236998328-5346d8b2-6298-4821-98d0-07f996a2715f.png)


**랜드마크 지정**

- 어떤 샘플을 랜드마크로 지정하면 좋은지 모름
- 따라서 모든 샘플을 랜드마크로 지정
- 장점: 차원이 커지면서 선형적으로 구분될 가능성 증가
- 단점: 훈련 셋이 매우 클 경우 동일한 크기의 아주 많은 특성이 생성

**SVC + 가우시안 RBF 커널**

- 유사도 특성을 실제론 추가하지 않으면서 수학적으로 추가한 효과 활용

```python
rbf_kernel_svm_clf = make_pipeline(StandardScaler(),
                                   SVC(kernel="rbf", gamma=5, C=0.001))
```

- 예제: moons 데이터셋
    
    ![Untitled 10](https://user-images.githubusercontent.com/91832324/236998332-92437211-7968-4b63-a45b-1d472bb746a1.png)


**SVM 분류 모델 계산 복잡도**

![Untitled 11](https://user-images.githubusercontent.com/91832324/236998334-2d3d7a73-4f85-44c4-8331-b9340113f847.png)

## 3. SVM 회귀

**SVM 분류 vs SVM 회귀**

- SVM 분류
    - 목표: 마진 오류 발생 정도를 조절($C$이용)하면서 결정 경계 도로의 폭을 최대한 넓게 하기
    - 마진 오류: 도로 위에 위치한 샘플
- SVM 회귀
    - 목표: 마진 오류 발생 정도를 조절($C$ 이용) 하면서 지정된 폭의 도로 안에 가능한 많은 샘플 포함하기
    - 마진 오류: 도로 밖에 위치한 샘플
    - 도로의 폭: `epsilon` 하이퍼파라미터로 지정

**선형 SVM 회귀**

- 예제: LinearSVR 활용. `epsilon`은 도로의 폭 결정

**`epsilon` 하이퍼 파라미터**

- 마진 안쪽, 즉 도로 위에 포함되는 샘플이 많아져도 예측에 영향 주지 않음.
이유는 마진 오류가 변하지 않기 때문
즉, `spsilon`만큼의 오차는 무시됨
- `epsilon`이 작을수록 도로폭이 좁아지기에 보다 많은 샘플을 마진 안쪽으로 포함시키기 위해 도로의 굴곡이 심해짐.
**따라서, `epsilon`이 클 수록 규제가 약해지는 효과 발생**

![Untitled 12](https://user-images.githubusercontent.com/91832324/236998336-7a4e94d5-af3c-4f79-a4a9-e5954ffe2d86.png)

**비선형 SVM 회귀**

- SVC와 동일한 커널 트릭을 활용해 비선형 회귀 모델 구현
- 예제: SVR + 다항 커널

```python
# SVR + 다항 커널
svm_poly_reg2 = make_pipeline(StandardScaler(),
                             SVR(kernel="poly", degree=2, C=100))
```

![Untitled 13](https://user-images.githubusercontent.com/91832324/236998337-7ee37449-f2f2-4cc4-b891-857275ea1d6a.png)

**SVM 회귀 모델 계산 복잡도**

- `LinearSVR`
    - `LinearSVR`와 유사
    - 시간 복잡도가 훈련 셋의 크기에 비례해 선형적으로 증가
- `SVR`
    - `SVC`와 유사
    - 훈련 셋이 커지면 매우 느려짐